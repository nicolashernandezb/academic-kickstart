+++
# Project title.
title = "Papers under review"

# Date this page was created.
date = 2016-04-27T00:00:00

# Project summary to display on homepage.
summary = "Kernel **depth measures** and **Domain Selection** for functional data."

# Tags: can be used for filtering projects.
# Example: `tags = ["machine-learning", "deep-learning"]`
tags = ["Depth Measures", "Domain Selection", "Divergence Function"]

# Optional external URL for project (replaces project detail page).
external_link = ""

# Featured image
# To use, add an image named `review.jpg/png` to your project's folder. 
[image]
  # Caption (optional)
#  caption = "Photo by rawpixel on Unsplash"
  
  # Focal point (optional)
  # Options: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight
  focal_point = "Smart"

+++


* **"Boosting classification performance for functional time series: A domain selection approach."**

Joint with [**Alberto Muñoz**](https://www.researchgate.net/profile/Alberto_Munoz9) & [**Gabriel Martos**](https://www.utdt.edu/ver_contenido.php?id_contenido=16862&id_item_menu=27721)

Submitted to: **_Journal of Automatica Sinica_**

<DIV align="justify">
In this paper we propose a novel domain selection methodology for functional time series. We introduce the divergence curve –and related concepts– as a tool to drop out redundant information in the context of supervised classification problems. The proposed method learns and infers about the sub–interval of the domain that better discriminates the classes of functions. Next we use the optimal selected domain to provide finite dimensional representations of the time series by functional
data analysis techniques. Simulations results show that the proposed methodology improves the classification performance
reducing, at the same time, the computational burden of several functional classification methods. We apply the methodology to several functional time series data sets and the empirical results show remarkable improvements in supervised classification when the effective domain is learned as a first step of the process.
</DIV>

**Keywords:** *Domain selection; Mutual Information; functional time series; Kullback–Leibler divergence; supervised learning.*

* **"Kernel depth measures for functional data with applications in functional outlier detection."**


Joint with [**Alberto Muñoz**](https://www.researchgate.net/profile/Alberto_Munoz9) & [**Gabriel Martos**](https://www.utdt.edu/ver_contenido.php?id_contenido=16862&id_item_menu=27721)

Submitted to: **_Pattern Recognition Journal_**

<DIV align="justify">
In the last years the concept of data depth has been increasingly used in Statistics and related fields, as a center-outward ordering metric for multivariate and functional data sets. Many of the functional measures operate directly on the raw representation of the data which somehow contradicts its functional nature and also presents some weaknesses. In this paper we introduce kernel depth measures for functional data, i.e. realizations of a stochastic process, represented in a Reproducing Kernel Hilbert Space. Through this representation, complex objects such as time series and images are transformed into points in finite dimensional Euclidean spaces. We propose a general depth measure, the Generalized Kernel Depth (GKD), valid for univariate and multivariate functional data, i.e. time series and images respectively. We prove that the proposed measure fulfils several desirable theoretical properties. Simulations results demonstrate that GKD works considerably better than other depth measures when the goal is to identify anomalous or outlier observations in non-Gaussian scenarios. Additionally we conduct several analyses of mortality rate curves, images processing, and biometric data as interesting applications of functional outlier detection. 
</DIV>

**Keywords:** *Kernel Depth, Functional data, RKHS, Outlier detection, Mortality curves, Image
processing, Biometric data.*

